{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e7edbc-3e01-4de7-bcfe-2da3a0ae01a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpunkt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PunktSentenceTokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core'"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def read_sentences(mddfile: str):\n",
    "    with open(mddfile) as handle:\n",
    "        lines = [line for line in handle.read().split('\\n') if len(line) > 0]\n",
    "    sentence_tokenizer = PunktSentenceTokenizer('\\n'.join(lines))\n",
    "    sentences = [\n",
    "        sentence.replace('\\n', ' ') for sentence in\n",
    "        sentence_tokenizer.sentences_from_text('\\n'.join(lines))\n",
    "    ]\n",
    "    return  sentences\n",
    "\n",
    "def read_docs(source):\n",
    "    sentences = read_sentences(source)\n",
    "    l_docs = []\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        doc = Document(sentence, metadata={'source': source, 'sentence_idx': idx})\n",
    "        l_docs.append(doc)\n",
    "    return l_docs\n",
    "\n",
    "def update_db(mddpath: str, dbpath: str, model_name: str = \"joe32140/ModernBERT-base-msmarco\"):\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    "    hf = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    db = Chroma(persist_directory=dbpath, embedding_function=hf)\n",
    "    for path in tqdm(glob(f'{mddpath}/*.md')):\n",
    "        docs = read_docs(path)\n",
    "        db.add_documents(docs)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef07882-9ab2-4f1b-93a5-4358b5582dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stivenr/NEUROREGEN/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/23/cqk3xkd960ldj9ytcfmhxjhc0000gn/T/ipykernel_51227/1861292963.py:35: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=dbpath, embedding_function=hf)\n",
      "Compiling the model with `torch.compile` and using a `torch.cpu` device is not supported. Falling back to non-compiled mode.\n",
      " 33%|████████████                        | 14/42 [3:57:05<24:18:44, 3125.89s/it]"
     ]
    }
   ],
   "source": [
    "db = update_db('mdd', 'bib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f23690-c7d1-436b-89d1-c4a39c259cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # vec_idxs = np.arange(len(sentences))\n",
    "    # to_drop = len(vec_idxs) % 3\n",
    "    # sentences = sentences[:-to_drop]\n",
    "    # vec_idxs = vec_idxs[:-3]\n",
    "    # docs = [\n",
    "    #     Document(' '.join([sentences[init_idx + i] for i in np.arange(3)]), metadata={'source': source})\n",
    "    #     for init_idx in vec_idxs\n",
    "    # ]\n",
    "    # return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cc3449e-d991-4c98-9b3c-a87ab48a790b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/23/cqk3xkd960ldj9ytcfmhxjhc0000gn/T/ipykernel_15598/858398884.py:2: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'line_idx': 30, 'source': 'mdd/@ferreVestibularContributionsRighthemisphere2015.md'}, page_content='For instance, vestibular and visual signals are integrated for perception of selfmotion.'), -404.9945684150278), (Document(metadata={'line_idx': 18, 'source': 'mdd/@ponzoVestibularModulationMultisensory2019.md'}, page_content='In such instances, the vestibular system, primarily involved in regulating balance and coordination during self‐motion, also contributes to multisensory integration, providing information signaling an unresolved conflict between vision (“I see motion”) and proprioception (“I feel I am not moving”), which often results in motion sickness (Bertolini & Straumann, 2016).'), -411.3483009113353), (Document(metadata={'line_idx': 350, 'source': 'mdd/@yakubovichVisualSelfmotionCues2020a.md'}, page_content='Furthermore, these impaired visual cues of self-motion are overweighted when integrated with largely intact vestibular cues, leading to suboptimal multisensory integration.'), -439.1498457581886), (Document(metadata={'line_idx': 31, 'source': 'mdd/@yakubovichVisualSelfmotionCues2020a.md'}, page_content='Self-motion perception relies primarily on vestibular and visual (optic flow) cues (Dichgans and Brandt, 1978; Warren and Hannon, 1988; Fushiki et al., 2005; Gu et al., 2007; Fetsch et al., 2009, 2010; Butler et al., 2010; Zaidel et al., 2015), as well as other somatosensory cues, such as proprioception (Probst et al., 1985; Hlava�cka et al., 1992; Mergner et al., 1993; Hlavacka et al., 1996; Mergner and Rosemeier, 1998; Schweigart et al., 2002; Durgin et al., 2005).'), -455.16548634843), (Document(metadata={'line_idx': 447, 'source': 'mdd/@yakubovichVisualSelfmotionCues2020a.md'}, page_content='Dynamic reweighting of visual and vestibular cues during self-motion perception.'), -467.2049892040563), (Document(metadata={'line_idx': 560, 'source': 'mdd/@yakubovichVisualSelfmotionCues2020a.md'}, page_content='Probst T, Straube A, Bles W. Differential effects of ambivalent visualvestibular-somatosensory stimulation on the perception of self-motion.'), -467.6900461568002), (Document(metadata={'line_idx': 52, 'source': 'mdd/@osobaBalanceGaitElderly2019.md'}, page_content='[8] The differing importance of visual input found in these two studies may be associated with Pyykko et al.’s[8] use of sway velocity to assess sensory inputs to balance while Judge et al.[16] measured center of force displacements and loss of balance events to assess sensory inputs to balance.'), -473.02347155996614), (Document(metadata={'line_idx': 6, 'source': 'mdd/@ponzoVestibularModulationMultisensory2019.md'}, page_content='We showed how vestibular signals modulate the weight of each sensory modality according to the context in which they are perceived and that such modulation extends to different aspects of tactile stimulation: felt and seen touch are differentially balanced in multisensory integration according to their epistemic relevance.'), -473.07258578829806), (Document(metadata={'line_idx': 338, 'source': 'mdd/@yakubovichVisualSelfmotionCues2020a.md'}, page_content='Here, we did not directly test the connection between impaired self-motion perception and gait and balance disorders.'), -473.5739566059887), (Document(metadata={'line_idx': 160, 'source': 'mdd/@peterkaSensoryIntegrationHuman2018.md'}, page_content='Given the complexity of individual components of sensory and motor systems contributing to balance control, it is incredible that this relatively simple model accounts quite well for body center-of-mass motion evoked by balance perturbations (Peterka, 2002, 2003; Cenciarini and Peterka, 2006).'), -477.545907978435), (Document(metadata={'line_idx': 121, 'source': 'mdd/@yakubovichVisualSelfmotionCues2020a.md'}, page_content='Also, coupling of the visual stimulus to inertial (vestibular) self-motion in combined trials, further heightened ----- the experience of self-motion from optic flow.'), -485.1565417679857), (Document(metadata={'line_idx': 438, 'source': 'mdd/@yakubovichVisualSelfmotionCues2020a.md'}, page_content='Self-motion perception during locomotor recalibration: more than meets the eye.'), -487.238587992229), (Document(metadata={'line_idx': 34, 'source': 'mdd/@yakubovichVisualSelfmotionCues2020a.md'}, page_content='In this study, we directly tested unisensory vestibular and unisensory visual performance, as well as multisensory integration of visual and vestibular cues for self-motion perception in Parkinson’s disease.'), -489.99291219074854), (Document(metadata={'line_idx': 38, 'source': 'mdd/@osobaBalanceGaitElderly2019.md'}, page_content='[10] ## Weighting of Sensory Inputs for Balance Many sensory systems contribute to balance, such as foot/ankle sensory input, visual input, and vestibular input.'), -491.64501470440945), (Document(metadata={'line_idx': 422, 'source': 'mdd/@yakubovichVisualSelfmotionCues2020a.md'}, page_content='Cullen KE. The vestibular system: multimodal integration and encoding of self-motion for motor control.'), -492.1625267535555), (Document(metadata={'line_idx': 155, 'source': 'mdd/@gorstLowerLimbSomatosensory2019.md'}, page_content='In essence, simple straight-line gait tasks may be completed using minimal somatosensory information and processing as visual feedback compensates.'), -493.5893425608081), (Document(metadata={'line_idx': 142, 'source': 'mdd/@bruijnControlHumanGait2018.md'}, page_content='The literature thus suggests that each of the three sensory modalities considered contributes to estimation of the CoM state and adjustment of ML foot placement to control stability, but this raises the question how multisensory information is integrated.'), -495.7368168783301), (Document(metadata={'line_idx': 26, 'source': 'mdd/@ventre-domineyVestibularFunctionTemporal2014.md'}, page_content='In order to differentiate between the motion of the visual surrounding versus self-motion, the central nervous system must integrate multimodal signals including visual and vestibular signals in order to extract the origin and direction of the perceived movement.'), -495.8170051351321), (Document(metadata={'line_idx': 8, 'source': 'mdd/@yakubovichVisualSelfmotionCues2020a.md'}, page_content='After experiencing vestibular (on a motion platform), visual (optic flow) or multisensory (combined visual–vestibular) self-motion stimuli at various headings, participants reported whether their perceived heading was to the right or left of straight ahead.'), -505.59634716165937), (Document(metadata={'line_idx': 186, 'source': 'mdd/@yakubovichVisualSelfmotionCues2020a.md'}, page_content='### Results In this study, we tested visual and vestibular (unisensory) self-motion perception, as well as visual–vestibular multisensory integration, in patients with Parkinson’s disease.'), -508.12141408346605), (Document(metadata={'line_idx': 354, 'source': 'mdd/@dellemonacheWatchingEffectsGravity2021.md'}, page_content='The vestibular system: multimodal integration and encoding [of self-motion for motor control.'), -509.5531499452265), (Document(metadata={'line_idx': 188, 'source': 'mdd/@ponzoVestibularModulationMultisensory2019.md'}, page_content='To conclude, we provide further evidence that the vestibular system may dynamically contribute to multisensory integration by weighting different sensory modalities according to the context in which they are experienced.'), -511.24284340166173), (Document(metadata={'line_idx': 100, 'source': 'mdd/@yakubovichVisualSelfmotionCues2020a.md'}, page_content='The (vestibular and visual) self-motion stimuli followed a linear path trajectory (0.13 m displacement) in the horizontal plane.'), -511.5947567740857), (Document(metadata={'line_idx': 380, 'source': 'mdd/@beylergilDoesVisuospatialMotion2022.md'}, page_content='Fetsch CR, Turner AH, DeAngelis GC, Angelaki DE (2009) Dynamic reweighting of visual and vestibular cues during self-motion perception.'), -511.8493048573034), (Document(metadata={'line_idx': 32, 'source': 'mdd/@yakubovichVisualSelfmotionCues2020a.md'}, page_content='Furthermore, these unisensory cues need to be integrated to form a unified and reliable percept of self-motion (Angelaki et al., 2009; Butler et al., 2010).'), -515.2363739605456), (Document(metadata={'line_idx': 89, 'source': 'mdd/@peterkaSensoryIntegrationHuman2018.md'}, page_content='These include: (1) reduction of variability and associated increase in certainty in neural representations of orientation and motion; (2) protection against sensory system dysfunction by virtue of having redundant sensory sources; (3) flexibility in organizing motor actions to meet specific behavioral goals; (4) resolving sensory conflicts by combining sensory cues to distinguish between self-motion and motion of the environment (e.g., moving surface or visual scene) or condition of the environment (e.g., firm or compliant surface); and (5) projecting sensory information across body segments via transformations that overcome limitations in individual sensory systems (i.e., combining head-referenced orientation information derived from visual and vestibular systems with head-on-body proprioception to derive body-in-space orientation information) (Mergner and Rosemeier, 1998).'), -515.6713672007543), (Document(metadata={'line_idx': 156, 'source': 'mdd/@bruijnControlHumanGait2018.md'}, page_content='Although some evidence suggests that the role of proprioception is much reduced in gait compared to stance [68], it has been observed that after sufficient habituation time, blindfolded individuals display a near-normal gait pattern [84], implying that stability can be maintained by relying on remaining sensory inputs.'), -517.4331782982639), (Document(metadata={'line_idx': 164, 'source': 'mdd/@bruijnControlHumanGait2018.md'}, page_content='The weighting of the sensory inputs likely depends on environmental conditions and the related change in reliability of the information provided by any single input, and is likely to vary across the gait cycle, but the dynamics of these weighting processes are largely unexplored.'), -518.3362240923036), (Document(metadata={'line_idx': 162, 'source': 'mdd/@bruijnControlHumanGait2018.md'}, page_content='These studies suggest that the effects of sensory inputs during gait are phase-dependent, but it is not clear whether and how these modulations are relevant to foot placement.'), -521.8610978801362), (Document(metadata={'line_idx': 332, 'source': 'mdd/@thomasKeepYourHead2020a.md'}, page_content='Utility of peripheral visual cues in planning and controlling adaptive gait.'), -521.9209153849412)]\n",
      "  results = db.similarity_search_with_relevance_scores(\n"
     ]
    }
   ],
   "source": [
    "query = 'What are the sensory modalities that contribute to self-motion perception, gait, and balance function?'\n",
    "results = db.similarity_search_with_relevance_scores(\n",
    "    query,\n",
    "    k=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93385737-b564-4cda-a62f-15db60a47023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For instance, vestibular and visual signals are integrated for perception of selfmotion. In such instances, the vestibular system, primarily involved in regulating balance and coordination during self‐motion, also contributes to multisensory integration, providing information signaling an unresolved conflict between vision (“I see motion”) and proprioception (“I feel I am not moving”), which often results in motion sickness (Bertolini & Straumann, 2016). Furthermore, these impaired visual cues of self-motion are overweighted when integrated with largely intact vestibular cues, leading to suboptimal multisensory integration. Self-motion perception relies primarily on vestibular and visual (optic flow) cues (Dichgans and Brandt, 1978; Warren and Hannon, 1988; Fushiki et al., 2005; Gu et al., 2007; Fetsch et al., 2009, 2010; Butler et al., 2010; Zaidel et al., 2015), as well as other somatosensory cues, such as proprioception (Probst et al., 1985; Hlava�cka et al., 1992; Mergner et al., 1993; Hlavacka et al., 1996; Mergner and Rosemeier, 1998; Schweigart et al., 2002; Durgin et al., 2005). Dynamic reweighting of visual and vestibular cues during self-motion perception. Probst T, Straube A, Bles W. Differential effects of ambivalent visualvestibular-somatosensory stimulation on the perception of self-motion. [8] The differing importance of visual input found in these two studies may be associated with Pyykko et al.’s[8] use of sway velocity to assess sensory inputs to balance while Judge et al.[16] measured center of force displacements and loss of balance events to assess sensory inputs to balance. We showed how vestibular signals modulate the weight of each sensory modality according to the context in which they are perceived and that such modulation extends to different aspects of tactile stimulation: felt and seen touch are differentially balanced in multisensory integration according to their epistemic relevance. Here, we did not directly test the connection between impaired self-motion perception and gait and balance disorders. Given the complexity of individual components of sensory and motor systems contributing to balance control, it is incredible that this relatively simple model accounts quite well for body center-of-mass motion evoked by balance perturbations (Peterka, 2002, 2003; Cenciarini and Peterka, 2006). Also, coupling of the visual stimulus to inertial (vestibular) self-motion in combined trials, further heightened ----- the experience of self-motion from optic flow. Self-motion perception during locomotor recalibration: more than meets the eye. In this study, we directly tested unisensory vestibular and unisensory visual performance, as well as multisensory integration of visual and vestibular cues for self-motion perception in Parkinson’s disease. [10] ## Weighting of Sensory Inputs for Balance Many sensory systems contribute to balance, such as foot/ankle sensory input, visual input, and vestibular input. Cullen KE. The vestibular system: multimodal integration and encoding of self-motion for motor control. In essence, simple straight-line gait tasks may be completed using minimal somatosensory information and processing as visual feedback compensates. The literature thus suggests that each of the three sensory modalities considered contributes to estimation of the CoM state and adjustment of ML foot placement to control stability, but this raises the question how multisensory information is integrated. In order to differentiate between the motion of the visual surrounding versus self-motion, the central nervous system must integrate multimodal signals including visual and vestibular signals in order to extract the origin and direction of the perceived movement. After experiencing vestibular (on a motion platform), visual (optic flow) or multisensory (combined visual–vestibular) self-motion stimuli at various headings, participants reported whether their perceived heading was to the right or left of straight ahead. ### Results In this study, we tested visual and vestibular (unisensory) self-motion perception, as well as visual–vestibular multisensory integration, in patients with Parkinson’s disease. The vestibular system: multimodal integration and encoding [of self-motion for motor control. To conclude, we provide further evidence that the vestibular system may dynamically contribute to multisensory integration by weighting different sensory modalities according to the context in which they are experienced. The (vestibular and visual) self-motion stimuli followed a linear path trajectory (0.13 m displacement) in the horizontal plane. Fetsch CR, Turner AH, DeAngelis GC, Angelaki DE (2009) Dynamic reweighting of visual and vestibular cues during self-motion perception. Furthermore, these unisensory cues need to be integrated to form a unified and reliable percept of self-motion (Angelaki et al., 2009; Butler et al., 2010). These include: (1) reduction of variability and associated increase in certainty in neural representations of orientation and motion; (2) protection against sensory system dysfunction by virtue of having redundant sensory sources; (3) flexibility in organizing motor actions to meet specific behavioral goals; (4) resolving sensory conflicts by combining sensory cues to distinguish between self-motion and motion of the environment (e.g., moving surface or visual scene) or condition of the environment (e.g., firm or compliant surface); and (5) projecting sensory information across body segments via transformations that overcome limitations in individual sensory systems (i.e., combining head-referenced orientation information derived from visual and vestibular systems with head-on-body proprioception to derive body-in-space orientation information) (Mergner and Rosemeier, 1998). Although some evidence suggests that the role of proprioception is much reduced in gait compared to stance [68], it has been observed that after sufficient habituation time, blindfolded individuals display a near-normal gait pattern [84], implying that stability can be maintained by relying on remaining sensory inputs. The weighting of the sensory inputs likely depends on environmental conditions and the related change in reliability of the information provided by any single input, and is likely to vary across the gait cycle, but the dynamics of these weighting processes are largely unexplored. These studies suggest that the effects of sensory inputs during gait are phase-dependent, but it is not clear whether and how these modulations are relevant to foot placement. Utility of peripheral visual cues in planning and controlling adaptive gait.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = ' '.join([result[0].page_content for result in results])\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb52f938-d485-45ac-9814-37cc989c0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06dfa3a-ecaa-4a8a-9018-568d8f6b8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n",
    "out = pipeline(f\"\"\"\n",
    "Answer the user query based on the following information:\n",
    "\n",
    "--- INFO BEGIN\n",
    "{context}\n",
    "--- INFO END\n",
    "\n",
    "--- QUERY BEGIN\n",
    "{query}\n",
    "--- QUERY END\n",
    "\n",
    "--- RESPONSE BEGIN\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a711e0b-86c5-49c5-a5ae-dc8c5f6f7c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8a74d4f-acd9-4fd3-8fb8-c63546d3ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d7e3ced-223d-4b1f-8a6c-d58838ddcfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the user query based on the following information:\n",
      "\n",
      "--- INFO BEGIN\n",
      "For instance, vestibular and visual signals are integrated for perception of selfmotion. In such instances, the vestibular system, primarily involved in regulating balance and coordination during self‐motion, also contributes to multisensory integration, providing information signaling an unresolved conflict between vision (“I see motion”) and proprioception (“I feel I am not moving”), which often results in motion sickness (Bertolini & Straumann, 2016). Furthermore, these impaired visual cues of self-motion are overweighted when integrated with largely intact vestibular cues, leading to suboptimal multisensory integration. Self-motion perception relies primarily on vestibular and visual (optic flow) cues (Dichgans and Brandt, 1978; Warren and Hannon, 1988; Fushiki et al., 2005; Gu et al., 2007; Fetsch et al., 2009, 2010; Butler et al., 2010; Zaidel et al., 2015), as well as other somatosensory cues, such as proprioception (Probst et al., 1985; Hlava�cka et al., 1992; Mergner et al., 1993; Hlavacka et al., 1996; Mergner and Rosemeier, 1998; Schweigart et al., 2002; Durgin et al., 2005). Dynamic reweighting of visual and vestibular cues during self-motion perception. Probst T, Straube A, Bles W. Differential effects of ambivalent visualvestibular-somatosensory stimulation on the perception of self-motion. [8] The differing importance of visual input found in these two studies may be associated with Pyykko et al.’s[8] use of sway velocity to assess sensory inputs to balance while Judge et al.[16] measured center of force displacements and loss of balance events to assess sensory inputs to balance. We showed how vestibular signals modulate the weight of each sensory modality according to the context in which they are perceived and that such modulation extends to different aspects of tactile stimulation: felt and seen touch are differentially balanced in multisensory integration according to their epistemic relevance. Here, we did not directly test the connection between impaired self-motion perception and gait and balance disorders. Given the complexity of individual components of sensory and motor systems contributing to balance control, it is incredible that this relatively simple model accounts quite well for body center-of-mass motion evoked by balance perturbations (Peterka, 2002, 2003; Cenciarini and Peterka, 2006). Also, coupling of the visual stimulus to inertial (vestibular) self-motion in combined trials, further heightened ----- the experience of self-motion from optic flow. Self-motion perception during locomotor recalibration: more than meets the eye. In this study, we directly tested unisensory vestibular and unisensory visual performance, as well as multisensory integration of visual and vestibular cues for self-motion perception in Parkinson’s disease. [10] ## Weighting of Sensory Inputs for Balance Many sensory systems contribute to balance, such as foot/ankle sensory input, visual input, and vestibular input. Cullen KE. The vestibular system: multimodal integration and encoding of self-motion for motor control. In essence, simple straight-line gait tasks may be completed using minimal somatosensory information and processing as visual feedback compensates. The literature thus suggests that each of the three sensory modalities considered contributes to estimation of the CoM state and adjustment of ML foot placement to control stability, but this raises the question how multisensory information is integrated. In order to differentiate between the motion of the visual surrounding versus self-motion, the central nervous system must integrate multimodal signals including visual and vestibular signals in order to extract the origin and direction of the perceived movement. After experiencing vestibular (on a motion platform), visual (optic flow) or multisensory (combined visual–vestibular) self-motion stimuli at various headings, participants reported whether their perceived heading was to the right or left of straight ahead. ### Results In this study, we tested visual and vestibular (unisensory) self-motion perception, as well as visual–vestibular multisensory integration, in patients with Parkinson’s disease. The vestibular system: multimodal integration and encoding [of self-motion for motor control. To conclude, we provide further evidence that the vestibular system may dynamically contribute to multisensory integration by weighting different sensory modalities according to the context in which they are experienced. The (vestibular and visual) self-motion stimuli followed a linear path trajectory (0.13 m displacement) in the horizontal plane. Fetsch CR, Turner AH, DeAngelis GC, Angelaki DE (2009) Dynamic reweighting of visual and vestibular cues during self-motion perception. Furthermore, these unisensory cues need to be integrated to form a unified and reliable percept of self-motion (Angelaki et al., 2009; Butler et al., 2010). These include: (1) reduction of variability and associated increase in certainty in neural representations of orientation and motion; (2) protection against sensory system dysfunction by virtue of having redundant sensory sources; (3) flexibility in organizing motor actions to meet specific behavioral goals; (4) resolving sensory conflicts by combining sensory cues to distinguish between self-motion and motion of the environment (e.g., moving surface or visual scene) or condition of the environment (e.g., firm or compliant surface); and (5) projecting sensory information across body segments via transformations that overcome limitations in individual sensory systems (i.e., combining head-referenced orientation information derived from visual and vestibular systems with head-on-body proprioception to derive body-in-space orientation information) (Mergner and Rosemeier, 1998). Although some evidence suggests that the role of proprioception is much reduced in gait compared to stance [68], it has been observed that after sufficient habituation time, blindfolded individuals display a near-normal gait pattern [84], implying that stability can be maintained by relying on remaining sensory inputs. The weighting of the sensory inputs likely depends on environmental conditions and the related change in reliability of the information provided by any single input, and is likely to vary across the gait cycle, but the dynamics of these weighting processes are largely unexplored. These studies suggest that the effects of sensory inputs during gait are phase-dependent, but it is not clear whether and how these modulations are relevant to foot placement. Utility of peripheral visual cues in planning and controlling adaptive gait.\n",
      "--- INFO END\n",
      "\n",
      "--- QUERY BEGIN\n",
      "What are the sensory modalities that contribute to self-motion perception, gait, and balance function?\n",
      "--- QUERY END\n",
      "\n",
      "--- RESPONSE BEGIN\n",
      "Answer the user query based on the following information: Self-motion perception involves both visual and\n"
     ]
    }
   ],
   "source": [
    "print(out[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e39ca-19c5-4dbe-80db-5bcbee029582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
