{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc8c454-bbb5-4fb9-aa5c-4ee801f5137c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stivenr/NEUROREGEN/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800fa7e-451f-49b0-a4a0-89761875f47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f688bcea-ed56-4e97-95d6-ada23039742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:40<00:00, 13.42s/it]\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "chatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7c141a-cd82-4b54-973b-d8e161bf37be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 μs, sys: 0 ns, total: 2 μs\n",
      "Wall time: 5.25 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'system',\n",
       "    'content': 'You are a pirate chatbot who always responds in pirate speak!'},\n",
       "   {'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': ' Arr matey! I be ye digital parrot, Captain Loggy the Talky. I be'}]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "chatbot(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd61156b-d464-4ee1-8dc6-f42f51a76135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m      \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           TextGenerationPipeline\n",
       "\u001b[0;31mString form:\u001b[0m    <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x147d48190>\n",
       "\u001b[0;31mFile:\u001b[0m           ~/NEUROREGEN/env/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Language generation pipeline using any `ModelWithLMHead`. This pipeline predicts the words that will follow a\n",
       "specified text prompt. When the underlying model is a conversational model, it can also accept one or more chats,\n",
       "in which case the pipeline will operate in chat mode and will continue the chat(s) by adding its response(s).\n",
       "Each chat takes the form of a list of dicts, where each dict contains \"role\" and \"content\" keys.\n",
       "\n",
       "Examples:\n",
       "\n",
       "```python\n",
       ">>> from transformers import pipeline\n",
       "\n",
       ">>> generator = pipeline(model=\"openai-community/gpt2\")\n",
       ">>> generator(\"I can't believe you did such a \", do_sample=False)\n",
       "[{'generated_text': \"I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I\"}]\n",
       "\n",
       ">>> # These parameters will return suggestions, and only the newly created text making it easier for prompting suggestions.\n",
       ">>> outputs = generator(\"My tart needs some\", num_return_sequences=4, return_full_text=False)\n",
       "```\n",
       "\n",
       "```python\n",
       ">>> from transformers import pipeline\n",
       "\n",
       ">>> generator = pipeline(model=\"HuggingFaceH4/zephyr-7b-beta\")\n",
       ">>> # Zephyr-beta is a conversational model, so let's pass it a chat instead of a single string\n",
       ">>> generator([{\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one word.\"}], do_sample=False, max_new_tokens=2)\n",
       "[{'generated_text': [{'role': 'user', 'content': 'What is the capital of France? Answer in one word.'}, {'role': 'assistant', 'content': 'Paris'}]}]\n",
       "```\n",
       "\n",
       "Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial). You can pass text\n",
       "generation parameters to this pipeline to control stopping criteria, decoding strategy, and more. Learn more about\n",
       "text generation parameters in [Text generation strategies](../generation_strategies) and [Text\n",
       "generation](text_generation).\n",
       "\n",
       "This language generation pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
       "`\"text-generation\"`.\n",
       "\n",
       "The models that this pipeline can use are models that have been trained with an autoregressive language modeling\n",
       "objective. See the list of available [text completion models](https://huggingface.co/models?filter=text-generation)\n",
       "and the list of [conversational models](https://huggingface.co/models?other=conversational)\n",
       "on [huggingface.co/models].\n",
       "\n",
       "Arguments:\n",
       "    model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
       "        The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
       "        [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
       "    tokenizer ([`PreTrainedTokenizer`]):\n",
       "        The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
       "        [`PreTrainedTokenizer`].\n",
       "    modelcard (`str` or [`ModelCard`], *optional*):\n",
       "        Model card attributed to the model for this pipeline.\n",
       "    framework (`str`, *optional*):\n",
       "        The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
       "        installed.\n",
       "\n",
       "        If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
       "        both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
       "        provided.\n",
       "    task (`str`, defaults to `\"\"`):\n",
       "        A task-identifier for the pipeline.\n",
       "    num_workers (`int`, *optional*, defaults to 8):\n",
       "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
       "        workers to be used.\n",
       "    batch_size (`int`, *optional*, defaults to 1):\n",
       "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
       "        the batch to use, for inference this is not always beneficial, please read [Batching with\n",
       "        pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
       "    args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
       "        Reference to the object in charge of parsing supplied pipeline parameters.\n",
       "    device (`int`, *optional*, defaults to -1):\n",
       "        Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
       "        the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
       "    torch_dtype (`str` or `torch.dtype`, *optional*):\n",
       "        Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
       "        (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
       "    binary_output (`bool`, *optional*, defaults to `False`):\n",
       "        Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
       "        the raw output data e.g. text.\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "Complete the prompt(s) given as inputs.\n",
       "\n",
       "Args:\n",
       "    text_inputs (`str`, `List[str]`, List[Dict[str, str]], or `List[List[Dict[str, str]]]`):\n",
       "        One or several prompts (or one list of prompts) to complete. If strings or a list of string are\n",
       "        passed, this pipeline will continue each prompt. Alternatively, a \"chat\", in the form of a list\n",
       "        of dicts with \"role\" and \"content\" keys, can be passed, or a list of such chats. When chats are passed,\n",
       "        the model's chat template will be used to format them before passing them to the model.\n",
       "    return_tensors (`bool`, *optional*, defaults to `False`):\n",
       "        Returns the tensors of predictions (as token indices) in the outputs. If set to\n",
       "        `True`, the decoded text is not returned.\n",
       "    return_text (`bool`, *optional*):\n",
       "        Returns the decoded texts in the outputs.\n",
       "    return_full_text (`bool`, *optional*, defaults to `True`):\n",
       "        If set to `False` only added text is returned, otherwise the full text is returned. Cannot be\n",
       "        specified at the same time as `return_text`.\n",
       "    clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to clean up the potential extra spaces in the text output.\n",
       "    continue_final_message( `bool`, *optional*): This indicates that you want the model to continue the\n",
       "        last message in the input chat rather than starting a new one, allowing you to \"prefill\" its response.\n",
       "        By default this is `True` when the final message in the input chat has the `assistant` role and\n",
       "        `False` otherwise, but you can manually override that behaviour by setting this flag.\n",
       "    prefix (`str`, *optional*):\n",
       "        Prefix added to prompt.\n",
       "    handle_long_generation (`str`, *optional*):\n",
       "        By default, this pipelines does not handle long generation (ones that exceed in one form or the other\n",
       "        the model maximum length). There is no perfect way to adress this (more info\n",
       "        :https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227). This provides common\n",
       "        strategies to work around that problem depending on your use case.\n",
       "\n",
       "        - `None` : default strategy where nothing in particular happens\n",
       "        - `\"hole\"`: Truncates left of input, and leaves a gap wide enough to let generation happen (might\n",
       "          truncate a lot of the prompt and not suitable when generation exceed the model capacity)\n",
       "    generate_kwargs (`dict`, *optional*):\n",
       "        Additional keyword arguments to pass along to the generate method of the model (see the generate method\n",
       "        corresponding to your framework [here](./text_generation)).\n",
       "\n",
       "Return:\n",
       "    A list or a list of lists of `dict`: Returns one of the following dictionaries (cannot return a combination\n",
       "    of both `generated_text` and `generated_token_ids`):\n",
       "\n",
       "    - **generated_text** (`str`, present when `return_text=True`) -- The generated text.\n",
       "    - **generated_token_ids** (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`) -- The token\n",
       "      ids of the generated text."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3ecd460-d1fd-4d36-b583-9762b05d91be",
   "metadata": {},
   "source": [
    "chunks = sentence_splitter.split_documents(documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
